{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CO416 - Machine Learning for  Imaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coursework 2 - Age regression from brain MRI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting age from a brain MRI scan can have diagnostic value for a number of diseases that cause structural changes and damage to the brain. Discrepancy between the predicted, biological age and the real, chronological age of a patient might indicate the presence of disease and abnormal changes to the brain. For this we need an accurate predictor of brain age which may be learned from a set of healthy reference subjects.\n",
    "The objective for the coursework is to implement two different supervised learning approaches for age regression from brain MRI. Data from 600 healthy subjects will be provided. Each approach will require a processing pipeline with different components that you will need to implement using methods that were discussed in the lectures and tutorials. There are dedicated sections in the Jupyter notebook for each approach which contain some detailed instructions, hints and notes.\n",
    "\n",
    "You may find useful ideas and implementations in the tutorial notebooks. Make sure to add documentation to your code. Markers will find it easier to understand your reasoning when sufficiently detailed comments are provided in your implementations.\n",
    "\n",
    "#### Read the descriptions and provided code cells carefully and look out for the cells marked with 'TASK'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started and familiarise ourselves with the data\n",
    "\n",
    "The following cells provide some helper functions to load the data, and provide some overview and visualisation of the statistics over the population of 600 subjects. Let's start by loading the meta data, that is the data containing information about the subject IDs, their age, and gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the meta data using pandas\n",
    "import pandas as pd\n",
    "\n",
    "data_dir = \"./data/brain/\"\n",
    "\n",
    "meta_data = pd.read_csv(data_dir + 'meta/clean_participant_data.csv')\n",
    "meta_data.head() # show the first five data entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some population statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.catplot(x=\"gender_text\", data=meta_data, kind=\"count\")\n",
    "plt.title('Gender distribution')\n",
    "plt.xlabel('Gender')\n",
    "plt.show()\n",
    "\n",
    "sns.distplot(meta_data['age'], bins=[10,20,30,40,50,60,70,80,90])\n",
    "plt.title('Age distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(range(len(meta_data['age'])),meta_data['age'], marker='.')\n",
    "plt.grid()\n",
    "plt.xlabel('Subject')\n",
    "plt.ylabel('Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a simple medical image viewer and import SimpleITK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact, fixed\n",
    "from IPython.display import display\n",
    "\n",
    "from utils.image_viewer import display_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imaging data\n",
    "\n",
    "Let's check out the imaging data that is available for each subject. This cell also shows how to retrieve data given a particular subject ID from the meta data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Subject with index 0\n",
    "ID = meta_data['ID'][0]\n",
    "age = meta_data['age'][0]\n",
    "\n",
    "# Data folders\n",
    "image_dir = data_dir + 'images/'\n",
    "image_filenames = glob.glob(image_dir + '*.nii.gz')\n",
    "\n",
    "mask_dir = data_dir + 'masks/'\n",
    "mask_filenames = glob.glob(mask_dir + '*.nii.gz')\n",
    "\n",
    "greymatter_dir = data_dir + 'greymatter/'\n",
    "greymatter_filenames = glob.glob(greymatter_dir + '*.nii.gz')\n",
    "\n",
    "\n",
    "image_filename = [f for f in image_filenames if ID in f][0]\n",
    "img = sitk.ReadImage(image_filename)\n",
    "\n",
    "mask_filename = [f for f in mask_filenames if ID in f][0]\n",
    "msk = sitk.ReadImage(mask_filename)\n",
    "\n",
    "greymatter_filename = [f for f in greymatter_filenames if ID in f][0]\n",
    "gm = sitk.ReadImage(greymatter_filename)\n",
    "\n",
    "print('Imaging data of subject ' + ID + ' with age ' + str(age))\n",
    "\n",
    "print('\\nMR Image (used in part A)')\n",
    "display_image(img, window=400, level=200)\n",
    "\n",
    "print('Brain mask (used in part A)')\n",
    "display_image(msk)\n",
    "\n",
    "print('Spatially normalised grey matter maps (used in part B)')\n",
    "display_image(gm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Volume-based regression using brain structure segmentation\n",
    "\n",
    "The first approach aims to regress the age of a subject using the volumes of brain tissues as features. The structures include grey matter (GM), white matter (WM), and cerebrospinal fluid (CSF). It is known that with increasing age the ventricles enlarge (filled with CSF), while it is assumed that grey and white matter volume may decrease over time. However, as overall brain volume varies across individuals, taking the absolute volumes of tissues might not be predictive. Instead, relative volumes need to be computed as the ratios between each tissue volume and overall brain volume. To this end, a four-class (GM, WM, CSF, and background) brain segmentation needs to be implemented and applied to the 600 brain scans. Brain masks are provided which have been generated with a state-of-the-art neuroimaging brain extraction tool.\n",
    "\n",
    "Different regression techniques should be explored, and it might be beneficial to investigate what the best set of features is for this task. Are all volume features equally useful, or is it even better to combine some of them and create new features. How does a simple linear regression perform compared to a model with higher order polynomials? Do you need regularisation? How about other regression methods such as regression trees or neural networks? The accuracy of different methods should be evaluated using two-fold cross-validation, and average age prediction accuracy should be compared and reported appropriately.\n",
    "\n",
    "*Note:* For part A, only the MR images and the brain masks should be used from the imaging data. The spatially normalised grey matter maps are used in part B only. If you struggle with task A-1, you can continue with A-2 using the provided reference segmentations in subfolder `segs_refs`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK A-1: Brain tissue segmentation\n",
    "\n",
    "Implement a CNN model for brain tissue segmentation which can provide segmentations of GM, WM, and CSF. For this task (and only for this task), we provide a separate dataset of 52 subjects which are split into 47 images for training and 5 for validation. The template code below has the data handling and main training routines already implemented, so you can focus on implementing a suitable CNN model. A simple model is provided, but this won't perform very well.\n",
    "\n",
    "Once your model is trained and you are happy with the results on the validation data you should apply it to the 600 test images. We provide reference segmentations in a subfolder `segs_refs` for all subjects. Calculate Dice similarity coefficients per tissue when comparing your predicted segmentations for the 600 test images to the reference segmentations. Summarise the statistics of the 600 Dice scores for each tissue class in [box-and-whisker-plots](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.boxplot.html).\n",
    "\n",
    "*Note:* Implementing a full-fledged machine learning pipeline with training and testing procedures in Jupyter notebooks is a bit cumbersome and a pain to debug. Also, running bigger training tasks can be unstable. The code below should work as is on your VM. However, if you want to get a bit more serious about implementing an advanced CNN approach for image segmentation, you may want to move code into separate Python scripts and run them from the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils.data_helper import ImageSegmentationDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check that the GPU is up and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_dev = '0' #GPU device 0 (can be changed if multiple GPUs are available)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:\" + cuda_dev if use_cuda else \"cpu\")\n",
    "\n",
    "print('Device: ' + str(device))\n",
    "if use_cuda:\n",
    "    print('GPU: ' + str(torch.cuda.get_device_name(int(cuda_dev))))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config and hyper-parameters\n",
    "\n",
    "Here we set some default hyper-parameters and a starting configuration for the image resolution and others.\n",
    "\n",
    "**This needs to be revisited to optimise these values. In particular, you may want to run your final model on higher resolution images.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_seed = 42 #fixed random seed\n",
    "\n",
    "img_size = [64, 64, 64]\n",
    "img_spacing = [3, 3, 3]\n",
    "\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "batch_size = 2\n",
    "val_interval = 10\n",
    "\n",
    "num_classes = 4\n",
    "\n",
    "out_dir = './output'\n",
    "\n",
    "# Create output directory\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and pre-processing of training and validation data\n",
    "\n",
    "We apply some standard pre-processing on the data such as intensity normalization (zero mean unit variance) and downsampling according to the configuration above.\n",
    "\n",
    "**We provide a 'debug' csv file pointing to just a few images for training. Replace this with the full training dataset when you train your full model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THIS FOR TRAINING ON ALL 47 SUBJECTS\n",
    "#train_data = data_dir + 'train/csv/train.csv'\n",
    "\n",
    "# USE THIS FOR DEBUGGING WITH JUST 2 SUBJECTS\n",
    "train_data = data_dir + 'train/csv/train_debug.csv'\n",
    "\n",
    "val_data = data_dir + 'train/csv/val.csv'\n",
    "\n",
    "print('LOADING TRAINING DATA...')\n",
    "dataset_train = ImageSegmentationDataset(train_data, img_spacing, img_size)\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print('\\nLOADING VALIDATION DATA...')\n",
    "dataset_val = ImageSegmentationDataset(val_data, img_spacing, img_size)\n",
    "dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise training example\n",
    "\n",
    "Just to check how a training image looks like after pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset_train.get_sample(0)\n",
    "img_name = dataset_train.get_img_name(0)\n",
    "seg_name = dataset_train.get_seg_name(0)\n",
    "print('Image: ' + img_name)\n",
    "display_image(sample['img'], window=5, level=0)\n",
    "print('Segmentation: ' + seg_name)\n",
    "display_image(sitk.LabelToRGB(sample['seg']))\n",
    "print('Mask')\n",
    "display_image(sample['msk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Model\n",
    "\n",
    "This is the **key part of task A-1** where you have to design a suitable CNN model for brain segmentation. The simple model provided below works to some degree (it let's you run through the upcoming cells), but it will not perform very well. Use what you learned in the lectures to come up with a good architecture. Start with a simple, shallow model and only increase complexity (e.g., number of layers) if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet3D(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleNet3D, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 4, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv3d(4, 8, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv3d(8, 4, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv3d(4, num_classes, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.conv4(x)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAINING\n",
    "\n",
    "Below is an implementation of a full training procedure including a loop for intermediate evaluation of the model on the validation data. Feel free to modify this procedure. For example, in addition to the loss you may want to monitor precision, recall and Dice scores (or others)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_dir = os.path.join(out_dir, 'model')\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "torch.manual_seed(rnd_seed) #fix random seed\n",
    "\n",
    "model = SimpleNet3D(num_classes=num_classes).to(device)\n",
    "model.train()\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_train_log = []\n",
    "loss_val_log = []\n",
    "epoch_val_log = []\n",
    "    \n",
    "print('START TRAINING...')\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "\n",
    "    # Training\n",
    "    for batch_idx, batch_samples in enumerate(dataloader_train):\n",
    "        img, seg = batch_samples['img'].to(device), batch_samples['seg'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        prd = model(img)\n",
    "        prd_flat = prd.view(prd.size(0), prd.size(1), -1)\n",
    "        seg_flat = seg.view(seg.size(0), seg.size(1), -1)\n",
    "        loss = F.cross_entropy(prd_flat, seg_flat.squeeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    loss_train_log.append(loss.item())\n",
    "\n",
    "    print('+ TRAINING \\tEpoch: {} \\tLoss: {:.6f}'.format(epoch, loss.item()))\n",
    "    \n",
    "    # Validation\n",
    "    if epoch == 1 or epoch % val_interval == 0:\n",
    "        loss_val = 0\n",
    "        sum_pts = 0\n",
    "        with torch.no_grad():\n",
    "            for data_sample in dataloader_val:\n",
    "                img, seg = data_sample['img'].to(device), data_sample['seg'].to(device)\n",
    "                prd = model(img)\n",
    "                prd_flat = prd.view(prd.size(0), prd.size(1), -1)\n",
    "                seg_flat = seg.view(seg.size(0), seg.size(1), -1)\n",
    "                loss_val += F.cross_entropy(prd_flat, seg_flat.squeeze(1), reduction='sum').item()\n",
    "                sum_pts += seg_flat.size(2)\n",
    "                \n",
    "        prd = torch.argmax(prd, dim=1)\n",
    "        prediction = sitk.GetImageFromArray(prd.cpu().squeeze().numpy().astype(np.uint8))\n",
    "        \n",
    "\n",
    "        loss_val /= sum_pts\n",
    "\n",
    "        loss_val_log.append(loss_val)\n",
    "        epoch_val_log.append(epoch)\n",
    "\n",
    "        print('--------------------------------------------------')\n",
    "        print('+ VALIDATE \\tEpoch: {} \\tLoss: {:.6f}'.format(epoch, loss_val))\n",
    "        display_image(sitk.LabelToRGB(prediction))\n",
    "        print('--------------------------------------------------')\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(model_dir, 'model.pt'))\n",
    "\n",
    "print('\\nFinished TRAINING.')\n",
    "\n",
    "plt.plot(range(1, num_epochs + 1), loss_train_log, c='r', label='train')\n",
    "plt.plot(epoch_val_log, loss_val_log, c='b', label='val')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and pre-processing of testing data\n",
    "\n",
    "Now that we have trained a model, the next cells are about applying that model to our test dataset. Before testing on the full 600 subjects, you may want to initially just test on the 5 validation subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THIS FOR TESTING ON THE 600 SUBJECTS\n",
    "#test_data = data_dir + 'csv/test.csv'\n",
    "\n",
    "# USE THIS FOR TESTING ON THE 5 VALIDATION SUBJECTS\n",
    "test_data = data_dir + 'train/csv/val.csv'\n",
    "\n",
    "print('LOADING TESTING DATA...')\n",
    "dataset_test = ImageSegmentationDataset(test_data, img_spacing, img_size)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise testing example\n",
    "\n",
    "Just to check how a testing image looks like after pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset_test.get_sample(0)\n",
    "img_name = dataset_test.get_img_name(0)\n",
    "seg_name = dataset_test.get_seg_name(0)\n",
    "print('Image: ' + img_name)\n",
    "display_image(sample['img'], window=5, level=0)\n",
    "print('Segmentation: ' + seg_name)\n",
    "display_image(sitk.LabelToRGB(sample['seg']))\n",
    "print('Mask')\n",
    "display_image(sample['msk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING\n",
    "\n",
    "Below is an implementation of a full testing procedure that saves the segmentations in an output folder. Feel free to modify this procedure.\n",
    "\n",
    "**You will need to add the calculations of Dice scores (and possibly others) to evaluate the segmentation performance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_dir = os.path.join(out_dir, 'pred')\n",
    "if not os.path.exists(pred_dir):\n",
    "    os.makedirs(pred_dir)\n",
    "\n",
    "model = SimpleNet3D(num_classes=num_classes)\n",
    "model.load_state_dict(torch.load(os.path.join(model_dir, 'model.pt')))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "    \n",
    "print('START TESTING...')\n",
    "\n",
    "loss_test = 0\n",
    "sum_pts = 0\n",
    "idx_test = 0\n",
    "with torch.no_grad():\n",
    "    for data_sample in dataloader_test:\n",
    "        img, seg = data_sample['img'].to(device), data_sample['seg'].to(device)\n",
    "        prd = model(img)\n",
    "        prd_flat = prd.view(prd.size(0), prd.size(1), -1)\n",
    "        seg_flat = seg.view(seg.size(0), seg.size(1), -1)\n",
    "        loss_test += F.cross_entropy(prd_flat, seg_flat.squeeze(1), reduction='sum').item()\n",
    "        sum_pts += seg_flat.size(2)        \n",
    "        \n",
    "        prd = torch.argmax(prd, dim=1)\n",
    "\n",
    "        sample = dataset_test.get_sample(idx_test)\n",
    "        name = dataset_test.get_seg_name(idx_test)\n",
    "        prediction = sitk.GetImageFromArray(prd.cpu().squeeze().numpy().astype(np.uint8))\n",
    "        prediction.CopyInformation(sample['seg'])\n",
    "        sitk.WriteImage(prediction, os.path.join(pred_dir, name))\n",
    "        \n",
    "        idx_test += 1\n",
    "        \n",
    "loss_test /= sum_pts\n",
    "\n",
    "print('+ TESTING \\tLoss: {:.6f}'.format(loss_test))\n",
    "\n",
    "# Show last testing sample as an example\n",
    "print('\\n\\nReference segmentation')\n",
    "display_image(sitk.LabelToRGB(sample['seg']))\n",
    "print('Predicted segmentation')\n",
    "display_image(sitk.LabelToRGB(prediction))\n",
    "\n",
    "print('\\nFinished TESTING.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK A-2: Feature calculation\n",
    "\n",
    "Start by calculating the three absolute tissue volumes for each subject. Plot the volumes against the subjects' ages. Taking the absolute volumes of tissues as features, however, might not be predictive. Instead, relative volumes need to be computed as the ratios between each tissue volume and overall brain volume. But you might also want to explore using different combinations or even polynomial features.\n",
    "\n",
    "Implement a function that constructs a big matrix $X$ with a row for each subject and features across the columns. Start with just calculating three simple features of relative tissue volumes for GM, WM and CSF, and compare these to the absolute volumes plotted above.\n",
    "\n",
    "*Note:* If you are struggling with the previous task on image segmentation, or if you prefer to work on this and the following tasks first, you can continue here using the provided reference segmentations which can be found in a subfolder `segs_refs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CALCULATE ABSOLUTE TISSUE VOLUMES\n",
    "\n",
    "import os\n",
    "\n",
    "# USE THIS TO RUN THE CALCULATIONS ON YOUR SEGMENTATONS\n",
    "#seg_dir = './output/pred/'\n",
    "\n",
    "# USE THIS TO RUN THE CALCULATIONS ON OUR REFERENCE SEGMENTATIONS\n",
    "seg_dir = data_dir + './segs_refs/'\n",
    "\n",
    "vols = np.zeros((3,meta_data['ID'].count()))\n",
    "\n",
    "for i in range(meta_data['ID'].count()):\n",
    "\n",
    "########################################\n",
    "# ADD YOUR CODE HERE\n",
    "########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot features versus age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# ADD YOUR CODE HERE\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CALCULATE RELATIVE TISSUE VOLUMES\n",
    "\n",
    "vols_normalised = np.zeros((3,meta_data['ID'].count()))\n",
    "\n",
    "########################################\n",
    "# ADD YOUR CODE HERE\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vols_normalised.T\n",
    "y = meta_data['age'].values.reshape(-1,1)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK A-3: Age regression and cross-validation\n",
    "\n",
    "Experiment with different regression methods from the [scikit-learn toolkit](http://scikit-learn.org/stable/supervised_learning.html#supervised-learning). Remember to construct the output vectur $y$ containing the age for each of the subjects.\n",
    "\n",
    "Evaluate the methods using two-fold [cross-validation](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation) where the dataset of 600 subjects is split into two equally sized sets $(X_1,y_1)$ and $(X_2,y_2)$ which are used for training and testing in an alternating way (so each set is used as $(X_{\\text{train}},y_{\\text{train}})$ and $(X_{\\text{test}},y_{\\text{test}})$ exactly once).\n",
    "\n",
    "Try using at least three different regression methods, and generate a plot allows easy comparison of the performance of the three methods. Useful [error metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics) to report include mean absolute error and r2 score. You might also want to plot the real vs predicted ages.\n",
    "\n",
    "*Note:* These [scikit-learn examples](https://scikit-learn.org/stable/auto_examples/) might serve as an inspiration.\n",
    "\n",
    "*Hint:* Be careful how you split the dataset into two folds. Take into account the data characteristics shown at the top of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# ADD YOUR CODE HERE\n",
    "########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Image-based regression using grey matter maps\n",
    "\n",
    "The second approach will make use of grey matter maps that have been already extracted from the MRI scans and aligned to a common reference space to obtain spatially normalised maps. For this, we have used an advanced, state-of-the-art neuroimaging toolkit, called SPM12. The reference space corresponds to the commonly used MNI atlas as seen in the lecture on image segmentation.\n",
    "\n",
    "Because these grey matter maps are spatially normalised (ie., registered), voxel locations across images from different subjects roughly correspond to the same anatomical locations. This means that each voxel location in the grey matter maps can be treated as an individual feature. Because those maps are quite large at their full resolution there would be a very large number of features to deal with (more than 850,000). A dimensionality reduction using PCA may need to be performed before training a suitable regressor on the low-dimensional feature representation obtained with PCA. It might also be beneficial to apply some pre-processing (downsampling, smoothing, etc.) before running PCA, which should be explored. The implemented pipeline should be evaluated using two-fold cross-validation using the same data splits as in part A, so the two different approaches can be directly compared in terms average age prediction accuracy.\n",
    "\n",
    "*Note:* For part B, only the spatially normalised grey matter maps should be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK B-1: Pre-processing\n",
    "\n",
    "Before running PCA to reduce the dimensionality of the feature space for grey matter maps, it might be beneficial to run some pre-processing on the maps. In voxel-based analysis where each voxel location is a feature, it is common to apply some smoothing beforehand. This is to reduce noise and to compensate for errors of the spatial normalisation that had been applied to the maps.\n",
    "\n",
    "Because the maps are quite large, it might also be worthwile to explore whether downsampling could be performed even before PCA. This would further reduce the dimensionality, and might be even needed in the case where PCA on the orignial resolution runs into memory issues. You may want to consider other ways of pre-processing and you can find insipiration in the notebook on medical image computing `MLI-MIC-Summary.ipynb`.\n",
    "\n",
    "Implement a function that performs suitable pre-processing on each grey matter map.\n",
    "\n",
    "*Hint:* You may want to save the pre-processed maps using `sitk.WriteImage` to avoid recomputation each time you run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# ADD YOUR CODE HERE\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = img_data #PRE-PROCESSED IMAGE DATA\n",
    "y = meta_data['age'].values.reshape(-1,1)\n",
    "\n",
    "print(img_size)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK B-2: Dimensionality reduction\n",
    "\n",
    "Implement dimensionality reduction for grey matter maps using [scitkit-learn's PCA](http://scikit-learn.org/stable/modules/decomposition.html#pca). PCA has an option to set the percentage of variance to be preserved (by setting the parameter `n_components` to a value between 0 and 1). The number of principal modes, that is the new dimensionality of the data, is then automatically determined. Try initially to preserve 95% of the variance (`n_components=0.95`).\n",
    "\n",
    "*Note:* When dimensionality reduction is used as pre-processing step for supervised learning, as in this case, it is important that PCA is fitted to the training data only, but then applied to both the training and testing data. So make sure your implementation consists of two separate steps, 1) fitting the PCA model to $X_{\\text{train}}$ (using the `fit` function), and 2) applying dimensionality reduction to $X_{\\text{train}}$ and $X_{\\text{test}}$ using the `transform` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# ADD YOUR CODE HERE\n",
    "########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK B-3: Age regression and cross-validation\n",
    "\n",
    "Experiment with different regression methods from the [scikit-learn toolkit](http://scikit-learn.org/stable/supervised_learning.html#supervised-learning). Evaluate the methods using two-fold [cross-validation](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation) in the same way as for your approach in Part A so results can be directly compared. Generate the similar plots.\n",
    "\n",
    "Try using at least three different regression methods.\n",
    "\n",
    "*Hint:* Remember, when you use cross-validation where you swap training and testing sets in each fold, you need to fit PCA to the training set of each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# ADD YOUR CODE HERE\n",
    "########################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
